# scripts/data_quality_check.py
# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

import sys
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import count, col, isnan, when, isnull, sum as spark_sum
from datetime import datetime, timedelta

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

def create_spark_session():
    """Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \
        .appName("DataQualityCheck") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:8020") \
        .getOrCreate()

def check_data_quality(spark, hdfs_path):
    """
    ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    - ë„ê°’ ê²€ì‚¬
    - ì¤‘ë³µ ë°ì´í„° ê²€ì‚¬
    - ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
    """
    try:
        df = spark.read.parquet(hdfs_path)
        total_records = df.count()
        
        if total_records == 0:
            logging.warning(f"ê²½ê³ : {hdfs_path}ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logging.info(f"ì´ {total_records}ê°œ ë ˆì½”ë“œ ê²€ì¦ ì¤‘...")
        
        # 1. ë„ê°’ ê²€ì‚¬
        null_counts = df.select([
            spark_sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c)
            for c in df.columns
        ]).collect()[0].asDict()
        
        # 2. ì¤‘ë³µ ê²€ì‚¬
        unique_records = df.distinct().count()
        duplicate_count = total_records - unique_records
        
        # 3. ê°€ê²© ë°ì´í„° ë¬´ê²°ì„± (ìŒìˆ˜ ê°€ê²© ê²€ì‚¬)
        invalid_prices = df.filter(col("price") <= 0).count() if "price" in df.columns else 0
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸
        logging.info("=== ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ===")
        logging.info(f"ì´ ë ˆì½”ë“œ: {total_records}")
        logging.info(f"ì¤‘ë³µ ë ˆì½”ë“œ: {duplicate_count}")
        logging.info(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê°€ê²©: {invalid_prices}")
        
        for col_name, null_count in null_counts.items():
            if null_count > 0:
                logging.warning(f"ë„ê°’ ë°œê²¬ - {col_name}: {null_count}ê°œ")
        
        # í’ˆì§ˆ ì„ê³„ê°’ ê²€ì‚¬
        quality_passed = (
            duplicate_count < total_records * 0.05 and  # ì¤‘ë³µ 5% ë¯¸ë§Œ
            invalid_prices == 0 and  # ìœ íš¨í•˜ì§€ ì•Šì€ ê°€ê²© 0ê°œ
            sum(null_counts.values()) < total_records * 0.1  # ë„ê°’ 10% ë¯¸ë§Œ
        )
        
        if quality_passed:
            logging.info("âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í†µê³¼")
        else:
            logging.error("âŒ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
            
        return quality_passed
        
    except Exception as e:
        logging.error(f"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    spark = create_spark_session()
    
    # ì›ë³¸ ë°ì´í„° ë° ë¶„ì„ ê²°ê³¼ ê²€ì¦
    raw_data_path = "hdfs://namenode:8020/user/spark/finnhub_market_data"
    analyzed_data_path = "hdfs://namenode:8020/user/spark/analyzed_finnhub_data"
    
    logging.info("ì›ë³¸ ë°ì´í„° í’ˆì§ˆ ê²€ì¦...")
    raw_quality_ok = check_data_quality(spark, raw_data_path)
    
    logging.info("ë¶„ì„ ê²°ê³¼ ë°ì´í„° í’ˆì§ˆ ê²€ì¦...")
    analyzed_quality_ok = check_data_quality(spark, analyzed_data_path)
    
    if raw_quality_ok and analyzed_quality_ok:
        logging.info("ğŸ‰ ì „ì²´ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ")
        sys.exit(0)
    else:
        logging.error("ğŸ’¥ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë°œê²¬")
        sys.exit(1)